When training computer vision models, one key step is developing something called visual code-words (also called a visual vocabulary). These are essentially compact, representative ‚Äúwords‚Äù that describe important image patterns.

Think of it like this: just as text documents are represented by words, images can be represented by visual words. These words come from patches of images where interesting stuff is happening ‚Äî corners, textures, or edges.


## üìë Table of Contents  

- [What is Machine Learning](#What-is-Machine-Learning)  
- [Unsupervised learning](#Unsupervised-learning)  
- [Reinforcement learning](#Reinforcement-learning)  
- [Supervised learning explanation](#Supervised-learning-explanation)
- [Supervised learning in Python](#Supervised-learning-in-Python)    

### **Step 1: Detecting Interest Points** 

The process starts by finding interest points in an image. These are locations where something meaningful is going on ‚Äî like a corner of a tire, an edge of a window, or a textured spot on the road.

- Common algorithms for finding interest points include:
- Harris Corner Detector
- SIFT (Scale-Invariant Feature Transform)
- SURF (Speeded-Up Robust Features)

Each interest point is surrounded by a small image patch that captures local texture or structure.
So, the first step in training, are developing what's called visual code-words.  And, basically the way it works is this, you have some sort of an operator, and  we'll talk about interest point operators,  that generate what are called interest points.  That is, these are points in the image where reasonable amounts of  interesting stuff is happening.  And we'll talk about Harris corners and other ways of finding that. 

<p align="center">
<img src="https://github.com/dr-mushtaq/Computer-Vision/blob/main/%F0%9F%93%9AChapter%2010%20L3%20Generalized%20Hough%20transform%20Edit%20section%20name/1.png"></a>
</p>

 ## **Step 2: Building Visual Code-Words**
 
Once we‚Äôve collected patches from many training images, we cluster them using an algorithm like k-means clustering.

- Each cluster center becomes a visual code-word.
- These code-words act like a dictionary that represents visual patterns.

For example:

- A small patch of a tire tread might form one code-word.
- A corner of a car window might form another code-word.
When combined, these code-words give us a structured way to describe and recognize objects.

What you do is, you take your interest point operator,  pull out all the interesting points on a bunch of training images.  You collect the little image patch right around those points,  you may get hundreds of them, or thousands of them, and then you cluster them,  and you use some algorithm for doing a clustering.  And when you're all done with those clusters,  the centers of those clusters are referred to, as visual code words.  So here, you can see all of these images that were  taken from something like tires, then they were all clustered, and so  the code word is this little tire piece. 

## **Step 3: Labeling Interest Points** 

Now, for every interest point in a new image:
- We compare its patch with the dictionary of visual code-words.
- We assign it the label of the code-word it matches best.
This way, each image can be described as a bag of visual words, similar to how text documents are represented by a bag of words in Natural Language Processing.

<p align="center">
<img src="https://github.com/dr-mushtaq/Computer-Vision/blob/main/%F0%9F%93%9AChapter%2010%20L3%20Generalized%20Hough%20transform%20Edit%20section%20name/333.png"></a>
</p>

Here's a, here is a centered tire, right?  This is the piece of a tire, this is a full tire.  And there are other kinds of code-words, so these become the little  features that we're going to look for in different images.  And this, of course, would assume in this particular case, we're looking for  cars, we got a bunch of training images on cars.  By the way I should say, all of this is done automatically, okay?  So, you're going to see some things that look a little strange in  terms of code-words, well.  It just happened to fall out of the data. 

<p align="center">
<img src="https://github.com/dr-mushtaq/Computer-Vision/blob/main/%F0%9F%93%9AChapter%2010%20L3%20Generalized%20Hough%20transform%20Edit%20section%20name/4444.png"></a>
</p>

This is not a human doing it.  This is telling the system to go ahead and do this.  The second thing you do is you take these code words, these are our features.  Remember like we had the tire, and  we found everywhere that the tire landed in the image.  So what we have here is all of these marks, or these little interest points.  And what we do is, for every interest point,  we find the feature that seems to look best at that point.  So that becomes the label of that point. 

All right, so remember just the way we had  a label before that said the gradient was horizontal pointing inward, here we  have the label is that it's the bottom right-hand corner of a tire, okay?  So this is mapping each of the interest points to some particular patch
<p align="center">
<img src="https://github.com/dr-mushtaq/Computer-Vision/blob/main/%F0%9F%93%9AChapter%2010%20L3%20Generalized%20Hough%20transform%20Edit%20section%20name/555.png"></a>
</p>

## **Step 4: Using Displacement Vectors** 

Finally, we don‚Äôt just store which code-word was found ‚Äî we also keep track of where it was found.

For example:

- If a tire code-word is detected on the left side of an image, we store a displacement vector pointing toward the car center.
- If the same code-word is found on the right side, the displacement vector points the other way.
This adds spatial context to our code-word representation, which helps in object detection

Finally, what we do is we take each of these little features and  we treat them just like we treated those little gradient images, right?  We take the patch, we find the displacement vector to the center, and we  write down that displacement vector in a table that's indexed by a patch label.  So if I find a tire and  it's to the left, which means the displacement vector is to the right.  I put down that displacement vector.  If I find a tire to the right which means its displacement vector is to  the left, I add that same displacement vector to the table with the entry  of the tire, and that stores all those displacements.

## **Why Visual Code-Words Matter** 

Visual code-words are powerful because they:

Provide a compact representation of images.

Work well for object recognition and scene classification.

Bridge ideas from text analysis into computer vision (bag-of-words model).

But keep in mind: modern deep learning often replaces this pipeline with convolutional neural networks (CNNs), which learn features automatically. Still, understanding visual code-words helps build intuition for feature-based vision methods.
 
 
### References

1-[Scikit Learn Tutorial](https://www.tutorialspoint.com/scikit_learn/index.htm)

2-[Scikit-Learn: A silver bullet for basic machine learning](https://medium.com/analytics-vidhya/scikit-learn-a-silver-bullet-for-basic-machine-learning-13c7d8b248ee)

4-[Machine Learning with scikit-learning (Datacamp)](https://www.datacamp.com/users/sign_in?redirect=http%3A%2F%2Fapp.datacamp.com%2Flearn%2Fcourses%2Fmachine-learning-with-scikit-learn)

5-[-Essential Python Libraries for Machine Learning and Data Science](https://www.deeplearning.ai/blog/essential-python-libraries-for-machine-learning-and-data-science/?utm_campaign=DLAI+Blog&utm_content=248986290&utm_medium=social&utm_source=facebook&hss_channel=fbp-1027125564106325)


<p align="center">
  <a href="#previous-section" style="text-decoration:none;">
    <button style="padding:20px 40px; font-size:24px; font-weight:bold; border-radius:12px; background-color:#007BFF; color:white; border:none; cursor:pointer;">
      ‚¨ÖÔ∏è Previous
    </button>
  </a>

  <a href="#next-section" style="text-decoration:none;">
    <button style="padding:20px 40px; font-size:24px; font-weight:bold; border-radius:12px; background-color:#28A745; color:white; border:none; cursor:pointer;">
      Next ‚û°Ô∏è
    </button>
  </a>
</p>






































